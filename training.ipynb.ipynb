{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    print(f\"Using device: cuda:0\")\n",
    "else:\n",
    "    print(\"Using device: cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_DIR = \"./telugu_tokenizer\"\n",
    "BASE_OUTPUT_DIR=\"./gpt-telugu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRICULUM_TXT=\"telugu_curriculum.txt\"\n",
    "TOKENS17_TXT=\"telugu_17M_tokens.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (GPT-Wee: very small)\n",
    "N_EMBD = 128\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 128\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "NUM_EPOCHS = 12\n",
    "LEARNING_RATE = 5e-4\n",
    "WARMUP_STEPS = 1000\n",
    "EVAL_STEPS = 5000\n",
    "SAVE_STEPS = 5000\n",
    "LOGGING_STEPS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_CHECKPOINTING=True\n",
    "DATALOADER_NUM_WORKERS=0\n",
    "DATALOADER_PIN_MEMORY=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = [42, 123, 456, 789, 1011, 2024, 3035, 4046, 5057, 6068]  \n",
    "NUM_RUNS_PER_MODE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(path):\n",
    "    \"\"\"Load text file line by line\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def load_tokenizer():\n",
    "    \"\"\"Load the pre-trained Telugu tokenizer\"\"\"\n",
    "    if not os.path.exists(TOKENIZER_DIR):\n",
    "        raise FileNotFoundError(f\"Tokenizer not found: {TOKENIZER_DIR}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = load_tokenizer()\n",
    "print(f\"Tokenizer loaded from: {TOKENIZER_DIR}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt2_wee(seed=6068, curriculum_mode=True, run_name=None):\n",
    "       random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Generate run name\n",
    "    mode_str = \"curriculum\" if curriculum_mode else \"random\"\n",
    "    if run_name is None:\n",
    "        run_name = f\"{mode_str}_seed_{seed}\"\n",
    "    \n",
    "    model_output_dir = os.path.join(BASE_OUTPUT_DIR, run_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "  \n",
    "    print(f\"Training: {run_name}\")\n",
    "    print(f\"Seed: {seed}, Mode: {mode_str}\")\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    curriculum_data = load_txt_file(CURRICULUM_TXT) if os.path.exists(CURRICULUM_TXT) else []\n",
    "    tokens17_data = load_txt_file(TOKENS17_TXT) if os.path.exists(TOKENS17_TXT) else []\n",
    "    \n",
    "    # Combine data based on mode\n",
    "    if curriculum_mode:\n",
    "        training_texts = curriculum_data + tokens17_data\n",
    "    else:\n",
    "        training_texts = curriculum_data + tokens17_data\n",
    "        random.shuffle(training_texts)\n",
    "    \n",
    "    # Create validation split (5% of data)\n",
    "    split_idx = int(0.95 * len(training_texts))\n",
    "    train_texts = training_texts[:split_idx]\n",
    "    val_texts = training_texts[split_idx:]\n",
    "    \n",
    "    # Limit validation size\n",
    "    if len(val_texts) > 2000:\n",
    "        val_texts = val_texts[:2000]\n",
    "    \n",
    "    print(f\"Training examples: {len(train_texts):,}\")\n",
    "    print(f\"Validation examples: {len(val_texts):,}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    raw_datasets = DatasetDict({\n",
    "        'train': Dataset.from_dict({'text': train_texts}),\n",
    "        'validation': Dataset.from_dict({'text': val_texts})\n",
    "    })\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=CONTEXT_LENGTH,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=True,\n",
    "        )\n",
    "        input_batch = []\n",
    "        for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "            input_batch.append(input_ids)\n",
    "        return {\"input_ids\": input_batch}\n",
    "    \n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    config = GPT2Config(\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_ctx=CONTEXT_LENGTH,\n",
    "        n_positions=CONTEXT_LENGTH,\n",
    "        n_embd=N_EMBD,\n",
    "        n_layer=N_LAYER,\n",
    "        n_head=N_HEAD,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.1,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        fp16=True if torch.cuda.is_available() else False,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    train_result = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = os.path.join(model_output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    # Save training stats\n",
    "    training_log_path = os.path.join(model_output_dir, \"training_stats.csv\")\n",
    "    log_df = pd.DataFrame(trainer.state.log_history)\n",
    "    log_df.to_csv(training_log_path, index=False)\n",
    "    \n",
    "    # Extract final metrics\n",
    "    log_history = trainer.state.log_history\n",
    "    final_train_loss = train_result.training_loss\n",
    "    final_eval_loss = None\n",
    "    final_eval_perplexity = None\n",
    "    \n",
    "    for entry in reversed(log_history):\n",
    "        if 'eval_loss' in entry:\n",
    "            final_eval_loss = entry['eval_loss']\n",
    "            if 'eval_perplexity' in entry:\n",
    "                final_eval_perplexity = entry['eval_perplexity']\n",
    "            break\n",
    "    \n",
    "    # Save run metadata\n",
    "    run_info = {\n",
    "        \"seed\": seed,\n",
    "        \"curriculum_mode\": curriculum_mode,\n",
    "        \"run_name\": run_name,\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"final_train_loss\": final_train_loss,\n",
    "        \"final_eval_loss\": final_eval_loss,\n",
    "        \"final_eval_perplexity\": final_eval_perplexity,\n",
    "        \"num_train_examples\": len(train_texts),\n",
    "        \"num_val_examples\": len(val_texts),\n",
    "        \"model_size_million\": sum(t.numel() for t in model.parameters()) / 1e6,\n",
    "        \"config\": {\n",
    "            \"context_length\": CONTEXT_LENGTH,\n",
    "            \"batch_size\": PER_DEVICE_BATCH_SIZE,\n",
    "            \"gradient_accumulation\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"n_embd\": N_EMBD,\n",
    "            \"n_layer\": N_LAYER,\n",
    "            \"n_head\": N_HEAD,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    run_info_path = os.path.join(model_output_dir, \"run_info.json\")\n",
    "    with open(run_info_path, \"w\") as f:\n",
    "        json.dump(run_info, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nTraining completed in {training_time/60:.2f} minutes\")\n",
    "    print(f\"Final train loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final eval loss: {final_eval_loss:.4f}\" if final_eval_loss else \"\")\n",
    "    print(f\"Results saved to: {model_output_dir}\\n\")\n",
    "    \n",
    "    return run_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_run_results():\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        run_name = f\"curriculum_seed_{seed}\"\n",
    "        run_dir = os.path.join(BASE_OUTPUT_DIR, run_name)\n",
    "        run_info_path = os.path.join(run_dir, \"run_info.json\")\n",
    "        \n",
    "        if os.path.exists(run_info_path):\n",
    "            with open(run_info_path, \"r\") as f:\n",
    "                run_info = json.load(f)\n",
    "                all_results.append(run_info)\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "def perform_statistical_analysis(df):    \n",
    "    results = {}\n",
    "    \n",
    "    # Metrics to analyze\n",
    "    metrics = ['final_eval_loss', 'final_train_loss', 'training_time_seconds']\n",
    "    if 'final_eval_perplexity' in df.columns:\n",
    "        metrics.append('final_eval_perplexity')\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        values = df[metric].dropna()\n",
    "        \n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        mean_val = values.mean()\n",
    "        std_val = values.std()\n",
    "        sem_val = std_val / np.sqrt(len(values))  # Standard error of mean\n",
    "        min_val = values.min()\n",
    "        max_val = values.max()\n",
    "        median_val = values.median()\n",
    "        \n",
    "        # Confidence intervals (95%)\n",
    "        if len(values) > 1:\n",
    "            ci = stats.t.interval(0.95, len(values)-1, loc=mean_val, scale=sem_val)\n",
    "        else:\n",
    "            ci = (mean_val, mean_val)\n",
    "        \n",
    "        results[metric] = {\n",
    "            'mean': mean_val,\n",
    "            'std': std_val,\n",
    "            'sem': sem_val,\n",
    "            'min': min_val,\n",
    "            'max': max_val,\n",
    "            'median': median_val,\n",
    "            'ci_lower': ci[0],\n",
    "            'ci_upper': ci[1],\n",
    "            'n': len(values),\n",
    "            'values': values.tolist()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_statistical_summary(analysis_results):\n",
    "    print(\"CURRICULUM LEARNING RESULTS SUMMARY\")\n",
    "    for metric, stats in analysis_results.items():\n",
    "        print(f\"\\n{metric.upper().replace('_', ' ')}\")\n",
    "        print(f\"  Mean:   {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
    "        print(f\"  Median: {stats['median']:.4f}\")\n",
    "        print(f\"  Range:  [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "        print(f\"  95% CI: [{stats['ci_lower']:.4f}, {stats['ci_upper']:.4f}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SEED_TO_RUN = 6068\n",
    "\n",
    "#print(f\"Running Curriculum Learning Experiment with seed {SEED_TO_RUN}...\")\n",
    "#result = train_gpt2_wee(seed=SEED_TO_RUN, curriculum_mode=True)\n",
    "#print(f\"Completed curriculum learning run with seed {SEED_TO_RUN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "\n",
    "def evaluate_perplexity(run_name):\n",
    "    \"\"\"\n",
    "    Calculate and save perplexity with improved error handling.\n",
    "    This version ensures perplexity is definitely saved.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(BASE_OUTPUT_DIR, run_name, \"final_model\")\n",
    "    run_info_path = os.path.join(BASE_OUTPUT_DIR, run_name, \"run_info.json\")\n",
    "    \n",
    "    # First, check if perplexity is already saved\n",
    "    if os.path.exists(run_info_path):\n",
    "        with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            run_info = json.load(f)\n",
    "            if \"final_eval_perplexity\" in run_info and run_info[\"final_eval_perplexity\"] is not None:\n",
    "                print(f\"✓ Perplexity already calculated for {run_name}: {run_info['final_eval_perplexity']:.4f}\")\n",
    "                return run_info['final_eval_perplexity']\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"✗ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(run_info_path):\n",
    "        print(f\"✗ run_info.json not found: {run_info_path}\")\n",
    "        print(\"  Cannot save perplexity without run_info.json\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Calculating perplexity for {run_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "        tokenizer_eval = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        # Reconstruct validation data (same as training)\n",
    "        curriculum_data = load_txt_file(CURRICULUM_TXT) if os.path.exists(CURRICULUM_TXT) else []\n",
    "        tokens17_data = load_txt_file(TOKENS17_TXT) if os.path.exists(TOKENS17_TXT) else []\n",
    "        \n",
    "        is_curriculum = \"curriculum\" in run_name\n",
    "        if is_curriculum:\n",
    "            training_texts = curriculum_data + tokens17_data\n",
    "        else:\n",
    "            seed = int(run_name.split(\"_\")[-1])\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            training_texts = curriculum_data + tokens17_data\n",
    "            random.shuffle(training_texts)\n",
    "        \n",
    "        split_idx = int(0.95 * len(training_texts))\n",
    "        val_data = training_texts[split_idx:]\n",
    "        if len(val_data) > 2000:\n",
    "            val_data = val_data[:2000]\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in val_data:\n",
    "                inputs = tokenizer_eval(text, return_tensors=\"pt\", truncation=True, max_length=CONTEXT_LENGTH)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                num_tokens = (input_ids != tokenizer_eval.pad_token_id).sum().item()\n",
    "                total_loss += loss.item() * num_tokens\n",
    "                total_tokens += num_tokens\n",
    "        \n",
    "        if total_tokens > 0:\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            perplexity = math.exp(avg_loss)\n",
    "        else:\n",
    "            perplexity = None\n",
    "        \n",
    "        # Save to run_info.json with robust error handling\n",
    "        print(f\"\\nSaving perplexity to {run_info_path}...\")\n",
    "        try:\n",
    "            # Read existing file\n",
    "            with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                run_info = json.load(f)\n",
    "            \n",
    "            # Update with perplexity\n",
    "            run_info[\"final_eval_perplexity\"] = perplexity\n",
    "            run_info[\"perplexity_eval_tokens\"] = total_tokens\n",
    "            run_info[\"perplexity_calculated_at\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            # Create backup\n",
    "            backup_path = run_info_path + \".backup\"\n",
    "            shutil.copy2(run_info_path, backup_path)\n",
    "            \n",
    "            # Write to temporary file first\n",
    "            temp_path = run_info_path + \".tmp\"\n",
    "            with open(temp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(run_info, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Replace original file\n",
    "            shutil.move(temp_path, run_info_path)\n",
    "            \n",
    "            # Verify it was saved correctly\n",
    "            with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                verify_info = json.load(f)\n",
    "                if \"final_eval_perplexity\" in verify_info:\n",
    "                    saved_perp = verify_info[\"final_eval_perplexity\"]\n",
    "                    if saved_perp == perplexity:\n",
    "                        print(f\"✓ Perplexity: {perplexity:.4f} (evaluated on {total_tokens:,} tokens)\")\n",
    "                        print(f\" Successfully saved and verified!\")\n",
    "                        # Remove backup if successful\n",
    "                        if os.path.exists(backup_path):\n",
    "                            os.remove(backup_path)\n",
    "                        return perplexity\n",
    "                    else:\n",
    "                        print(f\" Mismatch: Calculated {perplexity:.4f} but saved {saved_perp}\")\n",
    "                        # Restore backup\n",
    "                        shutil.move(backup_path, run_info_path)\n",
    "                else:\n",
    "                    print(f\" Perplexity key not found after saving\")\n",
    "                    # Restore backup\n",
    "                    shutil.move(backup_path, run_info_path)\n",
    "                    \n",
    "        except Exception as save_error:\n",
    "            print(f\"Error saving perplexity: {save_error}\")\n",
    "            print(f\"  Calculated perplexity: {perplexity:.4f}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Try to restore backup if it exists\n",
    "            if os.path.exists(backup_path):\n",
    "                shutil.move(backup_path, run_info_path)\n",
    "                print(f\"  Restored backup file\")\n",
    "        \n",
    "        return perplexity\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error calculating perplexity: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the fixed version\n",
    "run_to_check = \"curriculum_seed_456\"  # Change this to check other runs\n",
    "print(\"PERPLEXITY EVALUATION\")\n",
    "perplexity = evaluate_perplexity(run_to_check)\n",
    "\n",
    "if perplexity:\n",
    "    print(f\"FINAL RESULT: {run_to_check}\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "    \n",
    "    # Double-check it's in the file\n",
    "    run_info_path = os.path.join(BASE_OUTPUT_DIR, run_to_check, \"run_info.json\")\n",
    "    if os.path.exists(run_info_path):\n",
    "        with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            final_check = json.load(f)\n",
    "            if \"final_eval_perplexity\" in final_check:\n",
    "                print(f\"\\n Verified: Perplexity is now in run_info.json\")\n",
    "            else:\n",
    "                print(f\"\\n Warning: Still not in file after save attempt\")\n",
    "else:\n",
    "    print(f\"\\n Failed to calculate perplexity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load curriculum run results\n",
    "df_results = load_all_run_results()\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    # Display summary\n",
    "    print(\"SUMMARY:\")\n",
    "    print(df_results[['seed', 'final_train_loss', 'final_eval_loss', 'training_time_seconds']].to_string(index=False))\n",
    "    \n",
    "    # Save combined results\n",
    "    results_path = os.path.join(BASE_OUTPUT_DIR, \"curriculum_runs_summary.csv\")\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "else:\n",
    "    print(\"No completed runs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_results) > 0:\n",
    "    analysis_results = perform_statistical_analysis(df_results)\n",
    "    print_statistical_summary(analysis_results)\n",
    "    analysis_path = os.path.join(BASE_OUTPUT_DIR, \"statistical_analysis.json\")\n",
    "    with open(analysis_path, \"w\") as f:\n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_serializable(item) for item in obj]\n",
    "            elif isinstance(obj, (np.integer, np.floating)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        json.dump(convert_to_serializable(analysis_results), f, indent=2)\n",
    "    \n",
    "    print(f\"\\nStatistical analysis saved to: {analysis_path}\")\n",
    "else:\n",
    "    print(\"No results to analyze. Please run training experiments first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import math\n",
    "\n",
    "model_path = os.path.join(BASE_OUTPUT_DIR, \"curriculum_seed_1011\", \"final_model\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"text-generation\", model=model_path, tokenizer=model_path, device=device)\n",
    "\n",
    "# For minimal pairs, load model separately\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# 10 Test Prompts\n",
    "test_prompts = [\n",
    "    \"నేను పాఠశాలకు\",\n",
    "    \"ఆమె పేరు\",\n",
    "    \"నాకు నచ్చిన\",\n",
    "    \"ఈ రోజు\",\n",
    "    \"మా ఇంట్లో\",\n",
    "    \"అతను చాలా\",\n",
    "    \"తెలుగు భాష\",\n",
    "    \"పిల్లలు ఆడుకుంటున్నారు\",\n",
    "    \"భారతదేశంలో\",\n",
    "    \"ఒక రోజు\",\n",
    "]\n",
    "\n",
    "# 10 Minimal Pairs\n",
    "minimal_pairs = [\n",
    "    (\"అతను వెళ్ళాడు\", \"అతను వెళ్ళారు\"),\n",
    "    (\"ఆమె వచ్చింది\", \"ఆమె వచ్చాడు\"),\n",
    "    (\"నేను తింటున్నాను\", \"నేను తిన్నాను\"),\n",
    "    (\"రాముడు పుస్తకం చదివాడు\", \"రాముడు పుస్తకాన్ని చదివాడు\"),\n",
    "    (\"నేను వెళ్ళాను\", \"నేను వెళ్ళలేదు\"),\n",
    "    (\"నీవు వస్తావా\", \"నీవు వస్తావు\"),\n",
    "    (\"మీరు రండి\", \"నువ్వు రండి\"),\n",
    "    (\"ఇంటికి వెళ్ళాను\", \"ఇంటిలో వెళ్ళాను\"),\n",
    "    (\"తీసుకొని వచ్చాడు\", \"తీసుకొని వెళ్ళాడు\"),\n",
    "    (\"చదువుతున్నాను\", \"చదివాను\"),\n",
    "]\n",
    "\n",
    "print(\"TEXT GENERATION TEST\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    output = pipe(prompt, max_new_tokens=25, do_sample=True, temperature=1.0,\n",
    "                  repetition_penalty=2.0, no_repeat_ngram_size=2, top_k=30,\n",
    "                  pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "    generated = output[0]['generated_text'][len(prompt):].strip()\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {generated}\\n\")\n",
    "\n",
    "\n",
    "print(\"MINIMAL PAIR TEST\")\n",
    "\n",
    "for i, (sent1, sent2) in enumerate(minimal_pairs):\n",
    "    ppls = []\n",
    "    for sent in [sent1, sent2]:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            loss = model(inputs[\"input_ids\"], labels=inputs[\"input_ids\"]).loss.item()\n",
    "        ppls.append(math.exp(loss))\n",
    "    \n",
    "    print(f\"Pair {i+1}: '{sent1}' (PPL:{ppls[0]:.1f}) vs '{sent2}' (PPL:{ppls[1]:.1f}) → Lower: {'1st' if ppls[0]<ppls[1] else '2nd'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Collect perplexity values from all curriculum runs\n",
    "results = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    run_name = f\"curriculum_seed_{seed}\"\n",
    "    run_info_path = os.path.join(BASE_OUTPUT_DIR, run_name, \"run_info.json\")\n",
    "    \n",
    "    if os.path.exists(run_info_path):\n",
    "        with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            run_info = json.load(f)\n",
    "            \n",
    "            results.append({\n",
    "                'seed': seed,\n",
    "                'perplexity': run_info.get('final_eval_perplexity'),\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(BASE_OUTPUT_DIR, \"curriculum_perplexities.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved to: {csv_path}\")\n",
    "\n",
    "# Save to JSON\n",
    "json_path = os.path.join(BASE_OUTPUT_DIR, \"curriculum_perplexities.json\")\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved to: {json_path}\")\n",
    "\n",
    "# Display\n",
    "print(\"\\nPerplexity Results:\")\n",
    "print(df[['seed', 'perplexity']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"నేను పాఠశాలకు\",\n",
    "    \"ఆమె పేరు\",\n",
    "    \"నాకు నచ్చిన\",\n",
    "    \"ఈ రోజు\",\n",
    "    \"మా ఇంట్లో\",\n",
    "    \"అతను చాలా\",\n",
    "    \"తెలుగు భాష\",\n",
    "    \"పిల్లలు ఆడుకుంటున్నారు\",\n",
    "    \"భారతదేశంలో\",\n",
    "    \"ఒక రోజు\",\n",
    "]\n",
    "\n",
    "# Minimal pairs\n",
    "minimal_pairs = [\n",
    "    (\"అతను వెళ్ళాడు\", \"అతను వెళ్ళారు\"),\n",
    "    (\"ఆమె వచ్చింది\", \"ఆమె వచ్చాడు\"),\n",
    "    (\"నేను తింటున్నాను\", \"నేను తిన్నాను\"),\n",
    "    (\"రాముడు పుస్తకం చదివాడు\", \"రాముడు పుస్తకాన్ని చదివాడు\"),\n",
    "    (\"నేను వెళ్ళాను\", \"నేను వెళ్ళలేదు\"),\n",
    "    (\"నీవు వస్తావా\", \"నీవు వస్తావు\"),\n",
    "    (\"మీరు రండి\", \"నువ్వు రండి\"),\n",
    "    (\"ఇంటికి వెళ్ళాను\", \"ఇంటిలో వెళ్ళాను\"),\n",
    "    (\"తీసుకొని వచ్చాడు\", \"తీసుకొని వెళ్ళాడు\"),\n",
    "    (\"చదువుతున్నాను\", \"చదివాను\"),\n",
    "]\n",
    "\n",
    "# Storage for results\n",
    "all_generation_results = []\n",
    "all_minimal_pair_results = []\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Test all seeds\n",
    "for seed in SEEDS:\n",
    "    run_name = f\"curriculum_seed_{seed}\"\n",
    "    model_path = os.path.join(BASE_OUTPUT_DIR, run_name, \"final_model\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Skipping seed {seed} - model not found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    # Text Generation Test\n",
    "    print(\"\\nTEXT GENERATION:\")\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "    \n",
    "    for prompt_idx, prompt in enumerate(test_prompts):\n",
    "        output = pipe(prompt, max_new_tokens=25, do_sample=True, temperature=1.0,\n",
    "                     repetition_penalty=2.0, no_repeat_ngram_size=2, top_k=30,\n",
    "                     pad_token_id=tokenizer.eos_token_id)\n",
    "        generated = output[0]['generated_text'][len(prompt):].strip()\n",
    "        \n",
    "        all_generation_results.append({\n",
    "            'seed': seed,\n",
    "            'prompt_id': prompt_idx + 1,\n",
    "            'prompt': prompt,\n",
    "            'generated_text': generated\n",
    "        })\n",
    "        \n",
    "        print(f\"  {prompt_idx+1}. Prompt: {prompt}\")\n",
    "        print(f\"     Generated: {generated}\\n\")\n",
    "    \n",
    "    # Minimal Pair Test\n",
    "    print(\"\\nMINIMAL PAIRS:\")\n",
    "    for pair_idx, (sent1, sent2) in enumerate(minimal_pairs):\n",
    "        ppls = []\n",
    "        for sent in [sent1, sent2]:\n",
    "            inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                loss = model(inputs[\"input_ids\"], labels=inputs[\"input_ids\"]).loss.item()\n",
    "            ppls.append(math.exp(loss))\n",
    "        \n",
    "        all_minimal_pair_results.append({\n",
    "            'seed': seed,\n",
    "            'pair_id': pair_idx + 1,\n",
    "            'sentence1': sent1,\n",
    "            'sentence2': sent2,\n",
    "            'ppl1': ppls[0],\n",
    "            'ppl2': ppls[1],\n",
    "            'lower_ppl': 'sentence1' if ppls[0] < ppls[1] else 'sentence2',\n",
    "            'ppl_difference': abs(ppls[0] - ppls[1])\n",
    "        })\n",
    "        \n",
    "        print(f\"  Pair {pair_idx+1}: PPL1={ppls[0]:.1f}, PPL2={ppls[1]:.1f}, Lower={'1st' if ppls[0]<ppls[1] else '2nd'}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, tokenizer, pipe\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Save results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVING RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save generation results\n",
    "gen_df = pd.DataFrame(all_generation_results)\n",
    "gen_csv = os.path.join(BASE_OUTPUT_DIR, \"all_seeds_generation_results.csv\")\n",
    "gen_df.to_csv(gen_csv, index=False, encoding='utf-8')\n",
    "print(f\"Generation results saved: {gen_csv}\")\n",
    "\n",
    "gen_json = os.path.join(BASE_OUTPUT_DIR, \"all_seeds_generation_results.json\")\n",
    "with open(gen_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_generation_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Generation results saved: {gen_json}\")\n",
    "\n",
    "# Save minimal pair results\n",
    "pair_df = pd.DataFrame(all_minimal_pair_results)\n",
    "pair_csv = os.path.join(BASE_OUTPUT_DIR, \"all_seeds_minimal_pairs.csv\")\n",
    "pair_df.to_csv(pair_csv, index=False, encoding='utf-8')\n",
    "print(f\"Minimal pair results saved: {pair_csv}\")\n",
    "\n",
    "pair_json = os.path.join(BASE_OUTPUT_DIR, \"all_seeds_minimal_pairs.json\")\n",
    "with open(pair_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_minimal_pair_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Minimal pair results saved: {pair_json}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nGeneration Results: {len(all_generation_results)} entries\")\n",
    "print(f\"Minimal Pair Results: {len(all_minimal_pair_results)} entries\")\n",
    "print(f\"\\nSeeds tested: {sorted(set([r['seed'] for r in all_generation_results]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
