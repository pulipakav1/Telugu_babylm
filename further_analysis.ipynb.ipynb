{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cc4f47-1d6e-4558-b3e8-73d7940ada06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_OUTPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31742/1526119467.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load all results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgen_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all_seeds_generation_results.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpair_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_OUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all_seeds_minimal_pairs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BASE_OUTPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE RESULTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "# Load all results\n",
    "gen_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, \"all_seeds_generation_results.csv\"))\n",
    "pair_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, \"all_seeds_minimal_pairs.csv\"))\n",
    "\n",
    "# Load training perplexities\n",
    "perplexity_results = []\n",
    "for seed in SEEDS:\n",
    "    run_info_path = os.path.join(BASE_OUTPUT_DIR, f\"curriculum_seed_{seed}\", \"run_info.json\")\n",
    "    if os.path.exists(run_info_path):\n",
    "        with open(run_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            run_info = json.load(f)\n",
    "            perplexity_results.append({\n",
    "                'seed': seed,\n",
    "                'perplexity': run_info.get('final_eval_perplexity'),\n",
    "                'eval_loss': run_info.get('final_eval_loss'),\n",
    "                'train_loss': run_info.get('final_train_loss')\n",
    "            })\n",
    "perplexity_df = pd.DataFrame(perplexity_results)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"1. DATA OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Generation results: {len(gen_df)} entries ({len(gen_df['seed'].unique())} seeds)\")\n",
    "print(f\"Minimal pair results: {len(pair_df)} entries ({len(pair_df['seed'].unique())} seeds)\")\n",
    "print(f\"Training perplexities: {len(perplexity_df)} seeds\")\n",
    "print(f\"\\nSeeds in analysis: {sorted(gen_df['seed'].unique().tolist())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2d6d6-84d4-46b0-bee9-2912cfbeae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. TEXT GENERATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"2. TEXT GENERATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate generation length (in characters)\n",
    "gen_df['gen_length'] = gen_df['generated_text'].str.len()\n",
    "\n",
    "# Group by seed\n",
    "gen_summary = gen_df.groupby('seed').agg({\n",
    "    'gen_length': ['mean', 'std', 'min', 'max'],\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nGeneration Length Statistics (characters) by Seed:\")\n",
    "print(gen_summary)\n",
    "\n",
    "# Check for empty/very short generations\n",
    "empty_gens = gen_df[gen_df['gen_length'] < 5]\n",
    "if len(empty_gens) > 0:\n",
    "    print(f\"\\nâš  Warning: {len(empty_gens)} very short generations (< 5 chars)\")\n",
    "    print(empty_gens[['seed', 'prompt_id', 'generated_text']].head())\n",
    "\n",
    "# Average generation length across all seeds\n",
    "print(f\"\\nOverall average generation length: {gen_df['gen_length'].mean():.1f} characters\")\n",
    "print(f\"Overall std: {gen_df['gen_length'].std():.1f} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947073a2-5c1d-4149-b180-ce3d1bf85e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. MINIMAL PAIR CONSISTENCY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"3. MINIMAL PAIR CONSISTENCY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For each pair, check how consistent models are across seeds\n",
    "pair_consistency = pair_df.groupby('pair_id').agg({\n",
    "    'lower_ppl': lambda x: x.value_counts().to_dict(),\n",
    "    'ppl1': ['mean', 'std'],\n",
    "    'ppl2': ['mean', 'std'],\n",
    "    'ppl_difference': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nConsistency across seeds (which sentence has lower PPL):\")\n",
    "for pair_id in sorted(pair_df['pair_id'].unique()):\n",
    "    pair_data = pair_df[pair_df['pair_id'] == pair_id]\n",
    "    sent1_count = sum(pair_data['lower_ppl'] == 'sentence1')\n",
    "    sent2_count = sum(pair_data['lower_ppl'] == 'sentence2')\n",
    "    total = len(pair_data)\n",
    "    \n",
    "    print(f\"\\nPair {pair_id}:\")\n",
    "    print(f\"  Sentence 1 preferred: {sent1_count}/{total} ({sent1_count/total*100:.1f}%)\")\n",
    "    print(f\"  Sentence 2 preferred: {sent2_count}/{total} ({sent2_count/total*100:.1f}%)\")\n",
    "    print(f\"  Avg PPL difference: {pair_data['ppl_difference'].mean():.1f}\")\n",
    "\n",
    "# Most/least consistent pairs\n",
    "pair_agreement = []\n",
    "for pair_id in sorted(pair_df['pair_id'].unique()):\n",
    "    pair_data = pair_df[pair_df['pair_id'] == pair_id]\n",
    "    majority = pair_data['lower_ppl'].mode()[0] if len(pair_data['lower_ppl'].mode()) > 0 else 'tie'\n",
    "    agreement = sum(pair_data['lower_ppl'] == majority) / len(pair_data)\n",
    "    pair_agreement.append({\n",
    "        'pair_id': pair_id,\n",
    "        'agreement': agreement,\n",
    "        'majority_choice': majority\n",
    "    })\n",
    "\n",
    "agreement_df = pd.DataFrame(pair_agreement).sort_values('agreement')\n",
    "print(\"\\n\\nPair Consistency Ranking (higher = more consistent across seeds):\")\n",
    "print(agreement_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e7645-32cd-4535-94c3-cb6c45fb7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. CORRELATION ANALYSIS: Training Perplexity vs Test Performance\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"4. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Merge training perplexity with test results\n",
    "merged_df = perplexity_df.merge(\n",
    "    gen_df.groupby('seed')['gen_length'].mean().reset_index().rename(columns={'gen_length': 'avg_gen_length'}),\n",
    "    on='seed', how='inner'\n",
    ")\n",
    "\n",
    "# Calculate average minimal pair PPL difference per seed\n",
    "avg_pair_ppl_diff = pair_df.groupby('seed')['ppl_difference'].mean().reset_index()\n",
    "merged_df = merged_df.merge(avg_pair_ppl_diff, on='seed', how='inner')\n",
    "\n",
    "# Calculate correlation\n",
    "correlations = {\n",
    "    'perplexity_vs_gen_length': merged_df['perplexity'].corr(merged_df['avg_gen_length']),\n",
    "    'perplexity_vs_pair_diff': merged_df['perplexity'].corr(merged_df['ppl_difference']),\n",
    "    'eval_loss_vs_gen_length': merged_df['eval_loss'].corr(merged_df['avg_gen_length']),\n",
    "    'eval_loss_vs_pair_diff': merged_df['eval_loss'].corr(merged_df['ppl_difference'])\n",
    "}\n",
    "\n",
    "print(\"\\nCorrelations:\")\n",
    "for key, val in correlations.items():\n",
    "    print(f\"  {key}: {val:.3f}\")\n",
    "\n",
    "print(\"\\nMerged Data (Training Perplexity vs Test Metrics):\")\n",
    "print(merged_df[['seed', 'perplexity', 'eval_loss', 'avg_gen_length', 'ppl_difference']].to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
