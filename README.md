# Telugu Language Model (Research Project)

This project implements curriculum learning for training small GPT-2 models on Telugu text data. The project includes data preparation, tokenizer creation, model training, and comprehensive evaluation.

## Project Overview

The project trains GPT-2 "Wee" models (very small architecture) on Telugu text using curriculum learning. It includes:
- **Data preprocessing and curriculum creation** from Telugu news articles
- **Custom BPE tokenizer** trained on Telugu corpus
- **Multiple model training runs** with different random seeds for reproducibility
- **Comprehensive evaluation** including text generation and grammaticality judgments

## Project Structure

```
telugu_model/
├── Tokenizer_Telugu.ipynb              # BPE tokenizer creation
├── Telugu_Sentence_Scoring_2.ipynb     # Data preparation and curriculum creation
├── training.ipynb.ipynb                # Model training and evaluation
├── further_analysis.ipynb.ipynb        # Results analysis
├── curriculum_runs_summary.csv         # Training results summary
├── curriculum_perplexities.csv         # Perplexity metrics
├── all_seeds_generation_results.csv   # Text generation results
└── all_seeds_minimal_pairs.csv        # Minimal pair evaluation results
```

## Requirements

### Python Packages
- `transformers` - Hugging Face transformers library
- `torch` - PyTorch
- `datasets` - Hugging Face datasets
- `pandas`, `numpy` - Data manipulation
- `matplotlib`, `seaborn` - Visualization
- `scipy` - Statistical analysis
- `tokenizers` - Tokenizer library
- `indic-nlp-library` - Telugu text normalization
- `huggingface_hub` - Access to Hugging Face datasets

### Data Files
The project expects the following data files (generated by the notebooks):
- `telugu_curriculum.txt` - Curriculum-ordered training sentences
- `telugu_17M_tokens.txt` - Subset containing ~17M tokens
- `telugu_tokenizer/` - Directory containing the trained tokenizer

## Workflow

### 1. Tokenizer Creation (`Tokenizer_Telugu.ipynb`)

Creates a BPE (Byte Pair Encoding) tokenizer for Telugu text:
- Vocabulary size: 32,000
- Special tokens: `<pad>`, `<unk>`, `<bos>`, `<eos>`
- Trained on combined curriculum and 17M token datasets
- Output: `telugu_tokenizer/` directory

### 2. Data Preparation (`Telugu_Sentence_Scoring_2.ipynb`)

Processes Telugu news articles and creates curriculum-ordered training data:

**Steps:**
1. Downloads Telugu news articles from Hugging Face (`SuryaKrishna02/aya-telugu-news-articles`)
2. Text normalization using Indic NLP library
3. Cleaning: removes URLs, emails, HTML tags, non-Telugu symbols
4. Filters sentences by Telugu character ratio (>40%) and length (3-200 tokens)
5. Scores sentences based on:
   - Frame frequency (first 3 words)
   - Utterance length (shorter = easier)
   - Mean word length (shorter = easier)
   - Mean word frequency (higher = easier)
6. Orders sentences by difficulty (easy to hard)
7. Generates:
   - `telugu_curriculum.txt` - All ordered sentences (~214K sentences, ~17M tokens)
   - `telugu_17M_tokens.txt` - Subset with exactly 17M tokens

### 3. Model Training (`training.ipynb.ipynb`)

Trains GPT-2 "Wee" models with curriculum learning:

**Model Architecture:**
- Embedding dimension: 128
- Number of layers: 2
- Number of attention heads: 2
- Context length: 128 tokens
- Vocabulary size: 32,000 (from tokenizer)

**Training Configuration:**
- Batch size: 4 per device
- Gradient accumulation: 8 steps
- Effective batch size: 32
- Learning rate: 5e-4
- Epochs: 12
- Warmup steps: 1,000
- Evaluation steps: 5,000
- Optimizer: AdamW with cosine learning rate schedule

**Training Modes:**
- **Curriculum Learning**: Sentences ordered from easy to hard
- **Random Baseline**: Same data, randomly shuffled

**Multiple Runs:**
- 10 different random seeds: `[42, 123, 456, 789, 1011, 2024, 3035, 4046, 5057, 6068]`
- Each run saves:
  - Trained model in `gpt-telugu/{mode}_seed_{seed}/final_model/`
  - Training statistics in `training_stats.csv`
  - Run metadata in `run_info.json`

### 4. Evaluation

**Perplexity Evaluation:**
- Calculates perplexity on validation set
- Saves results to `run_info.json`

**Text Generation:**
- Tests 10 Telugu prompts
- Generates continuations with sampling
- Parameters: temperature=1.0, top_k=30, repetition_penalty=2.0

**Minimal Pair Testing:**
- Evaluates grammaticality judgments on 10 minimal pairs
- Compares perplexity of grammatically correct vs. incorrect sentences
- Tests linguistic knowledge (agreement, case marking, etc.)

### 5. Analysis (`further_analysis.ipynb.ipynb`)

Comprehensive analysis of results:
- Generation length statistics across seeds
- Minimal pair consistency analysis
- Correlation between training perplexity and test performance
- Statistical summaries with confidence intervals

## Output Files

### Training Results
- `curriculum_runs_summary.csv` - Summary of all training runs
- `curriculum_perplexities.csv` - Perplexity values for each seed
- `gpt-telugu/{run_name}/` - Individual model directories containing:
  - `final_model/` - Trained model and tokenizer
  - `training_stats.csv` - Training history
  - `run_info.json` - Run metadata and metrics

### Evaluation Results
- `all_seeds_generation_results.csv` - Text generation outputs for all seeds
- `all_seeds_minimal_pairs.csv` - Minimal pair perplexity comparisons

## Usage

### Running Training

1. **Create tokenizer** (if not already created):
   ```python
   # Run Tokenizer_Telugu.ipynb
   ```

2. **Prepare data** (if not already prepared):
   ```python
   # Run Telugu_Sentence_Scoring_2.ipynb
   ```

3. **Train models**:
   ```python
   # In training.ipynb.ipynb, uncomment and run:
   result = train_gpt2_wee(seed=42, curriculum_mode=True)
   ```

4. **Evaluate models**:
   ```python
   # Run evaluation cells in training.ipynb.ipynb
   ```

5. **Analyze results**:
   ```python
   # Run further_analysis.ipynb.ipynb
   ```

### Loading a Trained Model

```python
from transformers import GPT2LMHeadModel, AutoTokenizer, pipeline

model_path = "./gpt-telugu/curriculum_seed_42/final_model"
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# For text generation
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
output = pipe("నేను పాఠశాలకు", max_new_tokens=25)
```

## Results Summary

Based on the training runs:
- **Model Size**: ~4.5M parameters
- **Training Time**: ~2-2.5 hours per run (on GPU)
- **Final Perplexity**: ~109-110 on validation set
- **Training Loss**: ~4.83
- **Evaluation Loss**: ~4.69

## Key Features

1. **Curriculum Learning**: Sentences ordered from easy to hard for better learning
2. **Reproducibility**: Multiple seeds for statistical robustness
3. **Comprehensive Evaluation**: Both generation and grammaticality tests
4. **Telugu-Specific**: Custom tokenizer and normalization for Telugu text

## Notes

- The model architecture is intentionally very small ("GPT-2 Wee") for experimental purposes
- Training uses curriculum learning (easy-to-hard ordering) vs. random baseline
- All models are trained on the same data, only ordering differs
- Evaluation includes both quantitative (perplexity) and qualitative (generation, minimal pairs) metrics



